\chapter{Glossary}

\begin{description}

\item[Action potential] The rapid change in the membrane potential of a neuron, caused by a rapid flow of charged particles or ions across the membrane that occurs during the excitation of that neuron.

\item[Activation] Value associated with a node. Has different interpretations depending on the context. It can, for example, represent the firing rate of a neuron, or the presence of an item in working memory.

\item[Activation function] Function that converts weighted inputs into an activation in some node updated rules.

\item[Activation space] The set of all possible activation vectors for a neural network.

\item[Activation vector] A vector describing the activation values for a set of nodes in a neural network.

% \item[Adaptive exponential integrate and fire model]

\item[Anterograde amnesia] A type of memory loss or amnesia caused by brain damage in the hippocampus, where the ability to create new fact-based or declarative memories is compromised after the injury. 

\item[Artificial neural network] (Acronym: ANN) a collection of interconnected units, which processes information in a brain-like way.

\item[Attractor] A state or set of states with the property that any  sufficiently nearby state will go towards it. Fixed points and periodic orbits can be attractors.

\item[Ataxia] Impaired coordination or clumsiness caused by neurological damage in the cerebellum.

\item[Auditory cortex] Regions of the temporal lobes of the brain that process sound. 

\item[Auto-associator] A pattern associator that learns to associate vectors with themselves. In a recurrent network this can be used to model pattern completion. In a feed-forward network this can be used to test whether an input can be represented in a compressed form in the hidden layer and then recreated at the output layer. 

\item[Automatic process] A cognitive process that not require attention for its execution, and is relatively fast. Examples include riding a bike, driving a car, and brushing your teeth.

\item[Axon] The part of the neuron that carries outgoing signals to other neurons.

\item[Bag of words]: A representation method that associates a document with a vector of word frequencies, where each entry of the vector corresponds to a word and the value of the entry corresponds to the number of times that word occurs in the document. The underlying idea is that the word usage frequencies would capture the (semantic) content of a document. The order and grammatical information of the words in the document is disregarded, hence the term ``bag''.  A form of document embeddings, as contrasted with token or word embeddings, are more common with neural networks.

\item[Backpropagation] (Synonyms Backprop): A supervised learning algorithm that can train the weights of a multi-layer network using gradient extent. Can be thought of an extension of Least Mean Square methods for multi-layer networks. 

\item[Basal ganglia] A structure below the surface of the cortex (subcortical) that is involved in voluntary action and reward processing.

\item[Basin of attraction] The set of all states in a state space that tend 
towards a given attractor.

\item[Basis] A linearly independent set of vectors that span the whole vector space. Any two bases for a vector space have the same number of vectors.

\item[Bias] A fixed component of the weighted input to a node's activation. Determines its baseline activation when no inputs are received.

\item[Bifurcation] A topological change that occurs in a dynamical system as a parameter is varied.

\item[Binary vector] A vector all of whose components are $0$ or $1$.

\item[Biological neural network] A set of interconnected neurons in an animal brain.

\item[Bipolar vector] A vector all of whose components are $-1$ or $1$.

\item[Boolean functions] Functions that take a list of 0's and 1's as input and produce a 0 or 1 as output. The 0 represents a ``False'' and the 1 represents a ``True''. Boolean functions can be realized by logic gates.

\item[Brain stem] The lowest part of the brain that connects to the spinal cord and is fundamental for breathing, heart rate, and sleep.

\item[Broca's area] A region of the frontal lobe associated with language production, gesture, and speech. Damage to this area can lead to Broca's aphasia, affecting speech fluency.

\item[Categorical data] Data that can take one of a discrete set of values. For example, the time of day can be treated as a categorical variable taking two values: day and night. Also called nominal data.

\item[Cerebellum] A structure below the surface of the cortex (subcortical) involved in balance and fine movements, as well as maintaining internal models of the world for motor control.

\item[Cerebral cortex] The outer layer of the brain characterized by its structural folding (gyri and sulci); underlies complex behavior and intelligence in higher animals. 

\item[Chaotic dynamical system] A type of dynamical system in which the future behavior of the system is hard to predict. Such systems have sensitive dependence on initial conditions. Compare the ``butterfly effect.''

\item[Clamped node] A node that does not change during updating. Any activation function associated with the node is ignored, and its activation stays the same.

\item[Clamped weight] A weight that does not change during updating. Any local learning rule associated with the weight is ignored, and its strength stays the same.

\item[Classification task] A supervised learning task in which each input vector is associated with one or more discrete  categories. An example would be classifying images of faces as male or female. When classification associates each input with one category only, a one-hot encoding is often used on the output layer.

\item[Column vector] A vector whose components are written in a column \eg $\displaystyle \begin{pmatrix} 2 \\ 1 \\ 3 \end{pmatrix}$.

\item[Competitive learning] A form of unsupervised learning in which outputs nodes are trained to respond to clusters in the input space. 

\item[Computational neuroscience] The study of the brain using computer models.

\item[Computational cognitive neuroscience] The use of neural networks to simultaneously model psychological and neural processes.

\item[Connectionism] The study of psychological phenomena using artificial neural networks.

\item[Context window] The set of tokens that can be converted to vectors using a word embedding and sent as input to a transformer-based LLM.  The context window encompasses both user prompts and system responses; that is, both sides of a ``conversation'' with an LLM. Larger context windows allow LLMs to produce more coherent outputs and take account of more of a conversation.

\item[Convolutional layer] A special kind of weight layer where a set of weights (a ``filter'') is scanned over a previous node layer to produce activations in a target layer. This is related to the mathematical operation of convolution.

\item[Controlled process] A cognitive processes that requires attention for its execution, and is relatively slow. Examples include solving math problems, doing homework, or performing an  unusual task that you have not practiced.

\item[Cortical blindness] Neurological impairment caused by damage to the occipital lobe that results in blindness or inability to see. This can occur without any damage to the eyes. 

\item[Cross talk] A phenomenon where training patterns interfere with one another when training a neural network to perform some task. 

\item[Data cleaning] Removing, fixing, replacing, or otherwise dealing with bad data. Includes subsetting data, i.e. extracting rows or columns or removing rows or columns. One stage of data wrangling.

\item[Data science]  An area of science and practice concerned with managing and analyzing datasets, often using tools of machine learning, including neural networks.

\item[Data wrangling] (Synonyms Data munging, pre-processing): the process of transforming data into a form usable by a neural network. Encompasses obtaining, cleaning, imputing, coding, and rescaling data. 

\item[Dataset] Any table of numerical values that is used by a neural network, or that will be used by a neural network after pre-processing. (This is not a standard definition, but one stipulated in this text). Input, output, target, and training datasets are specific types of tables used in specific ways by neural networks.

\item[Decision boundary] In the context of a classification task, a hypersurface (e.g., in 2 dimensions, a line) that divides an input space into decision regions. Each decision region is associated with one possible output.

\item[Decision region] In the context of a classification task, a region of the input space associated with a specific class label. Any input that is in that region produces an output corresponding to that regions class label.

\item[Deep network] A neural network with a large number of successive layers of nodes mediating between inputs and outputs. Deep networks are trained using \emph{deep learning} techniques.

\item[Dendrite] The part of the neuron that receives signals from other neurons.

\item[Dendritic spine] Small outgrowths on the end of a dendritic branch where the receptors to which neurotransmitters attach can be found.

\item[Dimension of a vector space] The number of vectors in a basis for a vector space. This equals the number of components the vectors have. Examples: the line is a 1-dimensional vector space; the plane is a 2-dimensional vector space.

\item[Dimensionality reduction] A technique for transforming an $n$-dimensional vector space into another vector space with $m<n$ dimensions. A way of visualizing higher than 3-dimensional data that would otherwise be impossible to visualize.

\item[Discriminative model] A model that associates feature vectors, which are often distributed representations, with discrete categories (e.g. one-hot localist vectors).  Categories can be discriminated from distributed feature vectors. This is a non-standard, informal definition. The formal definition is that a discriminative model is a model of the conditional probability of categorical outputs given inputs. Contrasted with generative models.

\item[Distributed representation] A representation scheme where patterns of activation across groups of neurons indicate the presence of an object. 

\item[Dorsal stream] Pathway that extends from the occipital lobe into the parietal lobes, underlying visuospatial processing of visual objects in space. Damage to this pathway can cause impairment in reaching and grasping for objects. 

\item[Dot product] The scalar obtained by multiplying corresponding components of two vectors then adding the resulting products together. Example: $(1,2,3) \bullet (0,1,-1) = 0+2-3 = -1$.

\item[Dynamical system] A rule that says what state a system will be in at any future time, given any initial condition.

\item[Environment] A structure that influences the input nodes of a neural network or is influenced by the output nodes of a network, or both.

\item[Error function] A function that associates a network and a training dataset with a scalar error value. Many supervised learning techniques attempt to modify network parameters so as to reduce the error function. Also called a ``loss function'' or, in the context of mathematical optimization, an ``objective function.''

\item[Error surface] The graph of a function from parameters values of a network to error values of an associated error function. Each point on an error surface corresponds to different parameters (usually weight values and biases) of a network. Gradient descent finds minima on an error surface, where error is relatively low.

\item[Example] (Synonyms Instances, cases): rows of a dataset. Used in phrases like input example, training example, etc., depending on which dataset we are considering.

\item[Excitatory synapses] Synapses where an action potential in a presynaptic neuron triggers the release of neurotransmitters that then increase the likelihood of an action potential occurring in the postsynaptic neuron.

\item[Evolutionary algorithm] An algorithm that creates a model based on simulated evolution. In the context of neural networks, or ``evolved neural networks'', a set of randomly generated neural networks is created and they are used to perform some task.  Those that perform best are kept and combined with other top performers, and the resulting networks are used to perform the same task. The process is repeated over many generations. Closely related to genetic algorithms.

\item[Fan-in weight vector] The weight vector for all of the inputs to a node in a neural network.

\item[Fan-out weight vector] The weight vector for all of the outputs from a node in a neural network.

\item[Feature] (Synonyms Attribute, property): a column of a dataset, often associated with a node of a neural network.

\item[Feature Map] A node layer that is the output of a convolutional layer, in which each activation is the result of a filter being multiplied (using the dot product) to one region of the input layer.

\item[Feature-extraction] (Synonym Coding):  process of translating non-numerical data (e.g. text, images, audio files, DNA sequences) into a numerical format. % Using the hyphen because of the bug with overlap with "feature"

\item[Feed-forward network] A network comprised of a sequence of layers where all neurons in any layer (besides the last layer) are connected to all neurons in the next layer. Contains no recurrent connections.

\item[Firing rate] number of spikes (action potentials) A neuron fires per unit time. Usually measured in hertz, that is, number of spikes per second. A higher firing rate corresponds to a more ``active'' neuron. 

\item[Filter] (Synonym Kernel): the set of weights in a convolutional layer. This is more or less the same thing as a convolutional layer, but it refers specifically to the weights, whereas ``convolution'' also refers the process of passing the filter over the input activations.

\item[Filter Bank] A set of filters. One kind of volume-to-volume layer in a convolutional neural network. 

\item[Fixed point] A state that does not change under a dynamical system. The system ``stays'' in this state forever. An orbit consisting of a single point.

\item[Flatten] The process of converting a tensor with rank 3 or greater to a vector.  Allows the output of a convolutional layer to be sent to a standard feed-forward node layer.

\item[Frontal lobe] Forward-most lobe of the brain whose many roles include decision-making, action planning, and executive control. Houses many important regions, including prefrontal cortex (PFC), orbitofrontal cortex (OFC), and motor cortex. 

%\item[Function Approximation]

\item[Generalization] The ability of a neural network to perform tasks that were not included in its training dataset. An example would be a network that was trained to identify 10 faces as male, and 10 as female, being able to perform well on (or ``generalize to'') new faces it has not seen before.

\item[Generative AI] Artificially intelligent systems (often neural networks) that are capable of creating text, images, video, and other content, often at a level that is passably human.

\item[Generative Model] A model that can be used to generate prototypical features associated with some category, for example, associating a localist category label with a distributed feature vector. This is a non-standard definition. The formal definition is that it is a model of the joint probability distribution over a set of inputs and outputs. Contrasted with discriminative models. 

\item[Graceful degradation] A property of systems whereby decrease in performance is proportional to damage sustained. The contrast is with brittle systems, in which a small amount of damage can lead to complete failure.

\item[Gradient descent] A technique for finding a local minimum of (in a neural network context) an error function. Network parameters are iteratively updated using the negative of the gradient of the error function, which can be thought of as an arrow pointing in the direction in which the error surface is dropping most rapidly.

%\item[Hetero-associator] // Defunct, probbably don't need this.

\item[Hemineglect] Neurological impairment caused by damage to regions of the parietal lobes characterized by a lack of attending to anything in one half of the visual field.

\item[Hippocampus] A structure below the surface of the cortex (subcortical) that is involved in long-term memory consolidation and spatial maps. Damage to this structure can cause memory loss, or amnesia. 

%\item[Hodgkin-Huxley model]

\item[Hyperparameter] In the context of neural networks, a parameter that is not updated while a learning algorithm is applied. A learning rate is a hyperparameter, as is the size of a hidden layer, because both are set prior to training and are not updated during the training process.

\item[IAC network] A neural network used to model human semantic memory by spreading activation between pools of mutually inhibitory nodes that implement a winner-take-all or competitive structure. Weights are set by hand. 

\item[Inhibitory synapses] Synapses where an action potential in a presynaptic neuron triggers the release of neurotransmitters that then decrease the likelihood of an action potential occurring in the postsynaptic neuron.

\item[Imputation] The process of filling-in missing data in a data set. One stage of data wrangling.

\item[Initial condition] The state a dynamical system begins in.

\item[Input dataset] A dataset whose rows correspond to input vectors to be sent to the input nodes of a neural network.

\item[Input node] (Synonym Sensor): a node that takes in information from an external environment. 

\item[Input space] The vector space associated with the input layer of a neural network. The set of all possible input vectors for a neural network.

%\item[Integrate and Fire]

%\item[Inter-spike intervals]

%\item[Ionotropic]

%\item[Izhikevich model]

\item[Labeled dataset] A conjunction of two datasets: an input dataset with input vectors, and a target dataset with target vectors or ``labels''. An input-target dataset. Used for supervised learning tasks.  

\item[Large language model] (LLM) A model that generates language based on a large dataset.  LLMs are typically implemented as using the transformer architecture.

\item[Learning] In a neural network, a process of updating synaptic weights so that the network improves at producing some desired behavior relative to an error function or other objective function.

\item[Learning rate] A value that controls how much parameters are updated each time a learning rule is applied. Lower values lead to slower learning; larger values to faster learning.

\item[Learning rule] (in neural networks) A rule for updating the weights of a neural network. Application of this rule is sometimes called ``training.''

\item[Least mean square] (Synonym Delta rule): a supervised learning algorithm that adjusts weights and biases of a 2-layer feed-forward network so that input vectors in a training dataset produces outputs as similar as possible to corresponding target vectors.

\item[Linear activation function] A function that is typically just the weighted input, sometimes scaled by a slope parameter. A piecewise linear activation function clips weighted input at  upper and lower bounds. The ReLU function only clips at a lower bound of 0.

\item[Linear combination] To make a linear combination from a set of vectors we multiply each vector in the set by a scalar and then we add up the resulting vectors.

\item[Linearly dependent] A set of vectors is linearly dependent if there is at least one vector in the set can be expressed as a linear combination of the other vectors in the set.

\item[Linearly independent] A set of nonzero vectors that is not linearly
dependent is linearly independent.

\item[Linearly inseparable] A classification task that is not linearly separable.

\item[Linearly separable] A classification task can be solved using a decision boundary that is a line (or, in more than 2-dimensions, a plane or hyperplane).

\item[Logic gates] Devices that compute Boolean functions. For example, an AND gate has two inputs and one output. When both inputs are set to ``True'' the gate produces a ``True'' as output; otherwise the gate produces a ``False'' as output. Simple neural networks can implement logic gates.

\item[Localist representation] A representation scheme where activation of individual neurons indicate the presence of an object. Example: activation of neuron 25 indicates the presence of my grandmother.

\item[Long Term Depression] (LTD) A  process by which the efficacy of a synapse is decreased after repeated use.

\item[Long Term Potentiation] (LTP) A process by which the efficacy of a synapse is increased after repeated use. LTP is part of the basis of the Hebb rule.

\item[Machine learning] The use of statistical techniques to produce artificial intelligence. Uses of neural networks as engineering devices are a kind of machine learning.

\item[Matrix] A rectangular table of numbers. Often used to represents the 
weights of a neural network.

\item[Membrane potential] The voltage that results from the difference in the total sum charge of ions on either side of the cell membrane. A cell at rest typically has a resting membrane potential of -70 mV. 

%\item[Metabotropic]

\item[Motor cortex] Region of cortex that resides in very rear-most part of the frontal lobes that is responsible for the planning and execution of movement. 

\item[n-cycle] A finite set of $n$ states that a discrete-time dynamical system visits in the same repeating sequence. For discrete-time dynamical systems periodic orbits are $n$-cycles.

\item[Neuron] A cell in the nervous system specialized to transmit information.

\item[Neurotransmitters] Small chemical packages that transmit signals from one neuron to another via synapses. These packages are released when an action potential in a pre-synaptic neuron stimulates their release from vesicles on the axon terminals into the synaptic cleft where they travel to receptors on the dendrites of a post-synaptic neuron. 

\item[Node] (Synonyms Unit, artificial neuron): a simulated neuron or neuron-like element in an artificial neural network. 

\item[Node Layer] A collection of nodes that are treated as a group. For example, in a feed-forward network every node in one layer can be connected to every node in another layer. The activations in a node layer can be represented with an activation vector. Without qualification, ``layer'' means node layer.

\item[Numerical data] Data that is integer or real-valued. Examples include age, weight, and height. Data for a neural network must usually be converted into a numerical form.

\item[Occipital lobe] The lobe located in the back of the cortex where visual processing primarily takes place. 

\item[One-hot encoding] A one-of-$k$ encoding technique in which  a category with $k$ values is represented by a binary vector with $k$ components and the current value of the category corresponds to which nodes is on (or ``hot''). Example: representing cheap, moderate, and expensive restaurants with vectors $(1,0,0)$,$(0,1,0)$ and $(0,0,1)$. One-hot encodings are orthogonal t each other.
%For a category with $n$ members, we take an $n$-component zero-vector and then treat a 1 in the $k$-th column as representing the $k$-th category.

\item[Optimization] The process of finding the maximum or minimum of a function. In neural networks, it is often used to find network parameters for which error is lowest.

\item[Orbit] (Synonym Trajectory): the set of states visited by a dynamical system from an initial condition. 

\item[Orthogonal] Two vectors are orthogonal to each other if their dot product is zero. One-hot vectors are orthogonal. They are widely separated in input space and tend not to produce cross-talk in learning tasks.

\item[Orbitofrontal cortex] Front-most region of prefrontal cortex associated with decision-making.

\item[Output dataset] A dataset whose rows correspond to output vectors recorded from a neural network. 

\item[Output node] (Synonyms Actuator, effector): a node that provides information to an external environment. 

\item[Output space] The vector space associated with the output layer of a neural network. The set of all possible output vectors for a neural network.

\item[Padding] Entries added to an input in a convolutional layer in order to deal with issues relating to the edges of inputs. For example can be used to ensure width and height of the output remain the same.

\item[Parallel processing] Processing many items at once, concurrently. Contrasted with serial processing, where items are processed one at a time.  Neural networks are known for processing items in parallel, whereas classical computer process items in serial.

\item[Parameter] A quantity for a dynamical system that is fixed as the system runs but can be adjusted and run again with a different value. Used in the description of bifurcations. In a neural network, the parameters we  update are usually its weights and biases. This concept is also used in machine learning when treating a neural network as a trainable model, whose parameters (weights and biases) are updated using optimization techniques.

\item[Parietal lobe] The lobe located behind the frontal lobe and above the occipital lobe. This part of cortex plays a role in processing spatial information, integrating multisensory information, and is home to the somatosensory cortex, which processes information about touch sensation.

\item[Pattern associator] A neural network that associates each input vector in a set of input patterns with an output vector in a set of output (or ``target'') patterns. In most cases a pattern associator can be thought of as a vector-valued function. 
	
%\item[Pattern Classification] See classification task

\item[Pattern classifier] A pattern associator in which the output nodes are two-valued and are [interpreted as representing category membership. Example]: when the output node of a network is $1$, this means it's seeing a male face, when it is $0$ this means it's seeing a female face.

%\item[Pattern completion] // not sure... is this a task or network type? 

\item[Performance] Consideration of how a network responds to inputs, while keeping its weights fixed.  Often the term is also used to consider how well it is doing relative to an error function or objective function.

\item[Period of a periodic orbit] For a continuous time dynamical system the period is the time it takes the dynamical system to cover the periodic orbit. For a discrete time dynamical system the period is the number of points in the periodic orbit.

\item[Periodic orbit] A set of points that a dynamical system visits repeatedly and in the same order. An $n$-cycle is a type of periodic orbit.
% "limit cycle" is not synonymous with "perioidic orbit". 

\item[Phase portrait] A picture of a state space with important orbits draw in it. A picture of the dynamics of a system.

\item[Pooling layer] A volume-to-volume layer in a convolutional layer which reduces the amount of information passing through the network, ideally without altering the essential structure of that information. Related to subsampling and downsampling. 

\item[Positional encoding]: A method used in transformer models to add information about the position of tokens in a sequence to an activity pattern. This is important because transformers process information in parallel without knowledge of where a token occurs in a sequence.

\item[Pre-processing] The process of transforming data into a form usable by a neural network. Compare data-wrangling.

%\item[Point neurons]

\item[Prefrontal cortex] The front-most part of the frontal cortex, which is involved in executive function, decision-making, and planning. It is also thought to have an attractor-based structure that supports the operation of working memory.

\item[Primary motor cortex] Strip in the motor cortex that houses a somatotopic map of the body and controls simple movement production. 

\item[Proposition] (Synonyms Statement, sentence): an expression that can be true or false.

\item[Projection] A way of representing a group of points in a high-dimensional space in a lower dimensional space.

\item[Prosopagnosia] An impairment in recognizing faces that results from damage to particular regions of the ventral stream.

%\item[Rate-coding]

\item[Recurrent network] A network whose nodes are interconnected in such a way that activity can flow in repeating cycles.

\item[Receptors] Binding sites at the ends of dendrite branches of the post-synaptic neuron where neurotransmitters attach.

%\item[Refractory period]

\item[Regression task] A supervised learning task in which the goal is to create a network that produces outputs as close as possible to a set of target values. Targets are real-valued rather than binary (as they often are in classification tasks). An example would be predicting the exact price of a house based on its features.

\item[Reinforcement learning] A form of learning in which a system learns to take actions that maximize reward in the long run.   Actions that produce rewards, or action that lead to actions that produce reward, are reinforced in such a way that agents learn to obtain rewards and avoid costly situation.  In humans, associated with circuits in the brain stem and basal ganglia.  Sometimes treated as a third form of learning alongside supervised and unsupervised learning.

\item[ReLU activation function] A linear activation function that is clipped at  0. It's activation is 0 for weighted inputs less than or equal to 0, and it is equal to weighted inputs otherwise. It is a popular activation function for deep networks. Note that ``relu'' is short for ``rectified linear unit''.

\item[Repeller] (Synonym Unstable state): a state or set of states $R$ with the property that if the system is in a nearby state the system will always go away from R. Fixed points and periodic orbits can both be repellers.

\item[Representational depth] The number of node layers in a feed-forward network. More generally a description of the number of  layer-like structures stacked in a neural network. Deeper networks can produce more complex representations which aggregate representations of earlier layers. (The term ``depth'' also refers to a way of describing one component of the size a tensor, as in depth-by-width-by-height. That is a separate, unrelated use of the term).
 
\item[Representational width] The number of nodes in a node layer of a feed-forward network. More generally a description of the representational capacity of a layer or layer-like structure in a neural network. Wider layers can capture more features of the previous layer. This is non-standard terminology adopted in this book as a useful organizing principle. (The term ``width'' also refers to a way of describing one component of the size a tensor, as in depth-by-width-by-height. That is a separate, unrelated use of the term).

\item[Rescaling] A mathematical transformation of a set of samples in a dataset that preserves their relations to one another but changes their values. Often values are rescaled to lie in the interval $(0,1)$ or $(-1,1)$. One stage of data wrangling.

%\item[Reservoir Computing] 

\item[Retinotopic map] A topographic map of locations in the retina. Regions of the brain that are retinotopic maps have the property that neurons near one another process information about nearby areas in visual space.

\item[Row vector] A vector whose components are written in a row \eg $(2,1,3)$.

\item[Scalar] Usually a real number but in some applications it can be a complex number.When we multiply a vector by a scalar we are ``rescaling'' the vector, \ie changing the vector's length without changing its direction.

\item[Scalar multiplication] An operation used to ``rescale'' a vector. It takes a scalar and a vector and returns a vector with the same direction.

\item[Self Organizing Map] (Acronym SOM):  A network trained by unsupervised competitive learning, in which the layout of the output nodes corresponds to the layout of the input space.

\item[Sigmoid activation function] An activation function that whose value increases monotoically between a lower and upper bound. As the input goes infinitely far in the positive direction the value converges to the upper bound. As the input goes infinitely far in the negative direction the value converges to the lower bound.

%\item[Simple Recurrent Network]

\item[Soma] (Synonym Cell body): the central part of a neuron, which the dendrites and axons connect to.

\item[Softmax] A node layer which normalizes inputs so that its activations can be interpreted as a probability distribution. Each activation is between 0 and 1 and the sum over all the activations in a softmax layer is 1. 

\item[Somatotopic map] A topographic map in the somatosensory cortex that is organized by areas of the body. Nearby regions of this area represent nearby parts of the body.

\item[Somatosensory cortex] Front-most region of the parietal lobe that houses a somatotopic map of the body parts and processes tactile information from the body.

\item[Span] The set of all linear combinations of a set of vectors is called the span of that set of vectors.

\item[Spike] A discrete event that models the action potential for a neuron.

\item[State] A specification of values for all the variables describing a system. The state of a neural network is typically an activation vector.

\item[State space] The set of possible states of a system. The state spaces we consider are vector spaces. Two specific state spaces we focus on are activation spaces and weight spaces.

\item[State variable] A variable associated with a dynamical system that describes one number associated with a system at a time. Examples include a person's height and weight, a particle's position and momentum, and a neuron's activation. The \emph{state} of a system is a vector each of whose components is the value of one state variable. 

%\item[STDP Window]

\item[Strength] A value associated with a weight. Has different interpretations depending on the context. It can, for example, represent the efficacy of a synapse, or an association between items in memory.

\item[Stride] In a filter bank or pooling layer, the number of pixels the filter or kernel or pooling window is moved when it is scanned across its input.

%\item[Sum of Squared Error] 

\item[Sparse matrix] A matrix in which most of the entries are 0. A sparse weight matrix represents a set of connections between nodes where most of the possible connections are missing. Contrasted with dense matrices and dense or all-to-all connectivity. 

\item[Sparsity] Of a matrix is a number between $0$ and $1$ obtained by counting how many zero entries the matrix has and dividing by the total number of entries. A matrix with no zero entries has a sparsity of $0$ and a matrix with all zero entries has a sparsity of $1$.

\item[Subspace] Any subset of a vector space that also happens to satisfy the definition of a vector space. The sum of any two vectors in a subspace is in the subspace and any scalar multiple of a vector in a subspace is in the subspace.

\item[Supervised learning] A learning rule in which weights are adjusted using an explicit representation of desired outputs.

%\item[Supervised Recurrent Networks]

\item[Synapse] The junction between nerve cells where a information is transferred from one neuron to another.

\item[Synaptic efficacy] The degree to which a pre-synaptic spike increases the probability of a post-snyaptic spike at a synapse.

\item[Target dataset] A dataset whose rows correspond to target outputs we'd like a neural network to produce for corresponding input vectors. For classification tasks, a set of \emph{class labels}.

\item[Temporal lobe] Lobe forward of the occipital lobe and below the parietal and frontal lobes. This region is involved primarily in processing semantic information about what things are and factual information, and also houses several important language areas.

\item[Temporal lobe] Lobe forward of the occipital lobe and below the parietal and frontal lobes. This region is involved primarily in processing semantic information about what things are and factual information, and also houses several important language areas.

\item[Tensor] A generalization of the concept of a vector that encopasses numbers, lists of numbers, matrices, sets of matrices, sets of these sets, etc. The rank of a  tensor is the number of indices it takes to specify an ``entry'' in it.  A number is rank 0 because it requires no indices. A vector is rank 1 because it takes one index to specify an entry in a vector. A matrix is rank 2, because it takes two numbers to specify an entry (a row and column). A set of matrices is rank 3, because it takes 3 indices to specify an entry. Etc.

\item[Thalamus] An internal brain structure that relays information from sensory and motor structures to the cortex.

\item[Threshold potential] The membrane potential of the cell above which an action potential is fired. 

\item[Threshold activation function] A function that has one value for  input at or below a fixed amount (the threshold) and another value for input above the threshold. Usually the value of the function is less below the  threshold than it is above the threshold.

\item[Token] A word, word-part, or other sequence of characters. A generalization of the concept of a word to include other word-like entities. Tokens are associated with vectors in a word embedding.

\item[Token] A word, word-part, or other sequence of characters. A generalization of the concept of a word to include other word-like entities. Tokens are associated with vectors in a word embedding.

\item[Token embedding] (Near synonym: Word embedding). A way of associating each token or word in a document with a numerical vector. Useful in neural networks as a way to convert written text to network inputs. Modern LLMs utilize these token-level embeddings to deal with out-of-vocabulary terms, by breaking words down into sub-tokens. Many types of word embedding algorithm exist.

\item[Topology] The way the nodes and weights of a network are wired together. A network's ``architecture.''

\item[Tonotopic map] A topographic map in the auditory cortex that is organized by frequency of sounds. Similar sounds (in terms of frequency) are processed by neurons that are near one another.

\item[Transformer architecture] A complex feed-forward network structure which includes a ``self-attention'' mechanism that allows hidden layers to be ``aware'' of multiple kinds of relationships between different parts of a sequence of input activations. In the context of LLMs these networks can develop representations of words even when they are far apart in a document.

\item[Training subset] A subset of a dataset used for training a neural network model (or other machine learning model). Contrasted with testing subset.

\item[Truth table] A table whose columns are the inputs and outputs of a Boolean functions.

\item[Turing Test] The Turing Test is a behavioral test that can be used to determine whether a system is intelligent or not. The standard form of the test is as follows: If a human judge communicating with a system via a text interface cannot determine whether the system is human or not, the system passes the test and can be deemed intelligent.  The test derives from a similar test due to Alan Turing (the ``imitation game'') but is not identical with the imitation game. There is controversy surrounding whether the test is an adequate test of intelligence.

\item[Unsupervised learning] A learning rule in which weights are adjusted without an explicit representation of desired outputs.

\item[Vector] Ordered list of numbers ($n$-tuple of numbers). The numbers in a vector are its components. In many cases a vector represents a point. For example: $(2,2)$ is a vector with two components, which represents a point in a plane.

\item[Vector addition] (Synonym Vector sum): two vectors with the same number of components can be added (or summed) by adding their corresponding components. Example: $(1,2) + (3,4) = (4,6)$.

\item[Vector subtraction] Two vectors with the same number of components can be subtracted by subtracting their corresponding components.Example: $(1,2) - (3,4) = (-2,-2)$.

\item[Vector space] A collection of vectors, all of which have the same number of components. For example, the plane is a collection of vectors, all of which have two components. (Note that this is an  informal definition; to be a vector space, a set of vectors must meet further requirements as well).

\item[Vector-valued function] A function that takes vectors as inputs and produces vectors as outputs. (A more precise designation would be ``vector valued function of vector valued inputs''). 

\item[Ventral stream] Pathway that extends from the occipital lobe into the temporal lobes, underlying processing of visual object recognition. 

\item[Vesicles] The parts at the end of axon terminals where the neurotransmitters are stored for release. Upon triggering caused by action potentials, these vesicles will open and release the neurotransmitters into the synaptic cleft.

\item[Visual cortex] Rear-most region in the occipital lobe involved in visual processing, where primary and secondary visual cortex are housed.

\item[Weight] (Synonyms Connection, artificial synapse): a simulated synapse or synapse-like element in a neural network. 

\item[Weighted Input] (Synonym Net input): Dot product of an input vector and a fan-in weight vector, plus a bias term. Notated $n_i$ for neuron $i$.

\item[Weight layer] A set of weights treated as a group. Often they are the collection of weights connecting one node layer to another, which can in turn be represented by a weight matrix. % Distinguish recurrent from feed forward case?

\item[Weight space] The set of possible weight vectors for a given neural network.

\item[Weight vector] A vector describing the strengths of the weights in a neural network.

\item[Wernicke's area] A region of the temporal lobe associated with written and spoken language comprehension. Damage to this area can lead to Wernicke's aphasia, affecting the ability to understand language, even if language production remains intact.

\item[Winner-Take-All] A pool of nodes structured (often with mutually inhibitory connections) so that the node receiving the most inputs weighted inputs ``wins'' and becomes active while the other nodes become inactive.

\item[Zero vector] A vector whose components are all $0$. Adding the zero vector to any vector gives the same vector.

\end{description}