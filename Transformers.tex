\chapter{Transformers}\label{ch_transformers}
\chapterauthor{Jeff Yoshimi}

% Add glossary items for BPTT, SRN, auto-regression task, vanishing gradient, transformer, self-attention, etc.
% Better distinguish GPT variants

In this chapter we cover transformers and language models, which are associated with the explosion in interest in neural networks that came with the release of ChatGPT in 2022.  In this chapter we cover the basic architecture that is used to power systems like this, which are variously referred to as large language models, generative AI, or simply ``AI''. In particular, we cover the transformer architecture and the associated concept of self-attention.  This architecture builds on many of the insights of past chapters. It is ultimately just a really complex feed-forward supervised network running a form of backprop (chapters \extref{ch_supervised}, \extref{ch_lms_backprop}). It builds on progress made training supervised recurrent networks (chapter \extref{ch_supervised_recurrent}), but as we will see manages to use a feedforward architecture instead (though there is a kind of recurrence in how inputs are fed into it, as we will see).  It builds on the value of having many representations at each layers and many layers combining representations of earlier layers, the lesson of the deep learning revolution (chapter \extref{ch_deep_nets}).  They make use of huge amounts of data and highly optimized hardware, building on the broader deep learning revolution (section \extref{deep_revolution}). 

% TODO: Word embedding link
The key innovation with transformers is, in a sense, a special kind feature representation (compare the discussion of feature engineering in chapter \extref{ch_data_science}).  Words are converted into vectors using word embeddings, and sequences of these vectors are all fed into the network and compared. This gives the network a sense of context.

In this chapter we currently cover the basic idea of the architecture (not yet its details), and then consider how the transformer architecture is used in cognitive science as well, to identify learned internal representations that are psychologically meaningful. Change in this area is rapid, and the relevance of these areas to cognitive science is only now being studied, so updates to this the chapter are expected.

\section{The Transformer Architecture}\label{transformers}

Recurrent networks trained using backprop through time (like Karpathy's) are impressive but not perfect. The main theoretical problem they run into is that they favor recent context over earlier context.  But oftentimes it is important to be more sensitive to information much earlier in a sequence than more recent information, and as a result recurrent networks have a hard time capturing larger temporal structures,  like the plot of a story or the overall theme of a musical performance. There are also technical problems, like the the problem of vanishing gradients: as error is backpropagated through the weights of a network (section \extref{backprop}), the amount weights are changed further back in time gets smaller and smaller. Another problem is that this type of network does not lend itself to parallel computing hardware like GPUs on fancy graphics cards. This can be dealt with using some tricks, like truncating how far back the ``window'' of backprop goes (``truncated'' backprop through time), but this only takes us so far.
% In fact they bear some resemblance to those who suffer from antero-grade amnesia (cf. the piano performances of someone like Clive Wearing, who can create beautiful music but can no longer produce long pieces). Better develop this point with citations. 

One approach to this problem is to use nodes with more complex activations functions (see chapter \extref{ch_act_functions}).\footnote{Two such functions are ``long-short-term memories'' (LSTMs)  \cite{schmidhuber1997long, olah2015understanding} and ``gated recurrent units'' (GRUs). See \url{http://colah.github.io/posts/2015-08-Understanding-LSTMs.} These functions---which are like compound activation functions containing several others as parts---allow activations to be gated or turned off altogether. When placed in a recurrent network trained using gradient descent, they can learn to pass information far along a temporal sequence, ``jumping'' over intermediate nodes, in a way that avoids the problem of vanishing gradients.\label{lstm}} However, these approaches have not been discussed much in connection with cognitive science, and are being supplanted by other techniques, so we pass over them for now.

% Discuss online colab notebooks one can use to study these networks as well
A more powerful approach to training recurrent networks, which \emph{has} had an impact on cognitive science, is the transformer architecture \cite{vaswani2017attention}. Transformer networks are not recurrent networks--that are highly complex feed-forward networks--but they build directly on the idea of a recurrent network, and they do have some recurrent features, and so we discuss them in this chapter. We've seen that recurrent connections can provide a network with context information (for example with the copy-back layer of a SRN). What the network sees at time $t$ is an external input and some trace of what happened in the past.  Input plus context is enough for the network to learn to produce meaningful temporal sequences. Transformer networks improve on this basic idea. Rather than relying on recurrent connections for temporal context, they provide an entire sequence of words all at once. Thus during training the network can learn to find important contextual dependencies in these sequences, including long-range dependencies.

% A picture would help here
Suppose we want to ask a network ``hello how are you?'' In a recurrent network,  a vector representation (or ``vector embedding'') of each word in the sequence would be presented separately: ``hello'', ``how'', ``are'', and ``you''. In a transformer network, by contrast, we arrange these vector embeddings into a single matrix and present them all at once to the network. That is, the input to the network is the whole sentence $\{$``hello'', ``how'', ``are'', ``you''$\}$, in the form of a list of vectors, one for each word. There are a number of tricks used to represent this sentence in a useful way (recall the discussion of feature engineering in chapter \extref{ch_data_science})\footnote{In particular positional encodings, a special form of vector that can be added to a word embedding that tags it for its location in a sequence. Since words are intrinsically tagged with their position, they can be processed in parallel, which allows the architecture to run on fast parallel hardware.}, but for our purposes it is enough to just think of a whole sentence as an input matrix.

% Need example
% Say a bit more about details and link to ch. 4
This complex input encompassing an entire set of sentences is then sent through a specialized set of layers. Each layer combines  a ``self-attention'' layer with a traditional linear layer and several other mechanisms.  The self-attention layer is where the magic happens.  One part of this layer compares each word in the sentence to every other word in the sentence. So ``hello'' is compared to ``hello'', ``how'', ``are'', and ``you'', and this comparison is used to create an output representation of the sentence that reflects dependencies between the words. All words are compared to each other, so no matter how far apart they are in a sentence, they can influence each other. The attention layer learns what relations between words in a sentence are important; in a sense it learns what parts of the sentence other parts of the sentence should focus on (hence ``self attention'').  This ability to find meaningful relationships within an input sequence is part of why transformers are relevant to cognitive science.

Each self-attention layer of a transformer network can have multiple ``heads'';  it is a ``multi-headed'' attention mechanism.  As a result, the network can learn \emph{multiple} ways to compare words in the sentence to each other, a bit like how a convolutional network (section \extref{convolutionalLayer}) develops \emph{multiple} filters to analyze an image.  The results of these different attention heads are combined and as a result each layer of a transformer network involves a sophisticated representation of the sentence that represents multiple types of inter-word dependency.
% I really like Hedu AI's visual way of showing this. See \url{https://youtu.be/mMa2PmYJlCo}

Now we take a lesson from deep networks, and stack many of these transformer layers on top of each other.  When many of these attention layers are stacked together, we can get these extremely sophisticated representations.  Recall that with deep networks for vision, we get features, features of features, features of these features, etc. whose activations match neural response properties of different layers of the human visual system. This builds on the old idea of the \emph{Pandemonium} model (section \extref{cog_rev}), which involved (at successive layers): edge detectors, detectors for combinations of edges, detectors for combinations of these combinations (e.g. fragments of letters), and ultimately letter detectors. In a similar way, the successive layers of a transformer model of language correspond to increasingly complex features of the input stream, including syntactic categories, semantic properties, and far more complex features as well. We return to this topic at the end of the chapter, but note that understanding the nature of these representations is still in its infancy. The transformer model was not built to help cognitive science, after all, but to support NLP engineering tasks. Nonetheless, the results are so compelling that they are of interest to cognitive scientists.

Ok, so we have this super complex structure, that can represent multiple forms of inter-word dependency at multiple layers of a deep net representation. How do we train it?  Often using the basic auto-regressive ``one step-ahead'' prediction method we discussed at the start of the chapter. Just take a document---a big set of sentences---and send most of it as input to a transformer network, and ask it to predict what the very next word or token will be. This simple set up can be applied in a kind of recursive way to allow a network to learn to produce long responses to simple prompts.\footnote{This is easiest to see in a video; see \url{https://youtu.be/gJ9kaJsE78k?t=546}.}  However, they can also be trained in other ways. For example, we can feed a transformer network the first halves of hundreds of movie scripts and train it to produce the second halves of those scripts, or we can train it to predict what the first summary paragraph of a Wikipedia article is based on the main body of the article. Either way, we use gradient descent (section \extref{sect_gradient_descent}).  When the network predicts the wrong next word (or the wrong second half of a script, or the wrong first paragraph of a Wikipedia article), all the many weights in all these transformer layers are adjusted, so that the next time it tries to predict the same word or words relative to the same prompt, it does a little better. Repeat this many times, and the network will become amazingly good at ``talking'' like a human. Of course, it is slow, but the structural features of transformer networks make them work well with modern parallel hardware.
% Feed first half of hundreds of movies and train it to complete them

\section{Bert}\label{sect_bert}

Transformer networks have been extremely successful, as we saw in the discussion of GPT-3. They have also revolutionized natural language processing, via the BERT language model, which is making an impact in the cognitive sciences, especially in the computational study of language. In a paper co-authored by Jay McClelland  \cite{mcclelland2020placing} (a member of the original PDP group at UCSD; see section \extref{first_resurgence}), the following example is used to illustrate the point:
\begin{quote}
John put some beer in a cooler and went out with his friends to play volleyball. Soon after he left, someone took the beer out of the cooler. John and his friends were thirsty after the game, and went back to his place for some beers. When John opened the cooler, he discovered that the beer was \rule{1cm}{0.15mm}.
\end{quote}
The reader expects the word ``gone'' next, but if ``took the beer'' is replaced with ``took the ice'' the reader expects something like ``warm''. But the first part of the sentence is too far apart from the last part for an unrolled backprop through time network to pick up the relationship.  Transformer networks like BERT overcome this problem.

The big questions are: how exactly does BERT do this, and whatever it does, is it relevant to human language processing?  This creates a fascinating twist on our earlier discussion of types of neural network research (section \extref{typesOfResearch}). Recall that neural networks are sometimes used for engineering, sometimes for science, but that there is a feedback between these categories of research. For example, deep convolutional networks originated in scientific studies of visual processing, got refined and used for pattern recognition tasks in engineering, and the resulting deep networks were so powerful that they were then taken back into science  to describe (for example) the human visual system.
Additionally, the training task for BERT differs from other contemporary language models; BERT uses ``masked language modeling'' (MLM) instead of traditional next word prediction.\footnote{See \cite{devlin2018bert} for a detailed explanation.} Instead of modeling language as a left-to-right stream of words, where the model predicts the next word based on the previous context, BERT instead masks a token in the middle of the sentence, and predicts it based on the surrounding sentential context. This is advantageous over unidirectional left-to-right and concatenated left-to-right and right-to-left models, since it creates a truly bidirectional representation where the left and right contexts are joined together.

Here we see a variant on that pattern. In this case, recurrent neural networks (with their own long history spanning cognitive science and engineering) weren't performing well enough, so engineers created these new transformer architectures, mainly to tackle engineering issues like vanishing gradients and parallelization.  They used the intuitive concept of attention, but it was all engineering. BERT came out of Google and fit their engineering needs. Psychologists and linguists then realized  BERT was doing better at analyzing language than other models in linguistics, so they started to treat it as an object of scientific interest in its own right. This gave rise to a new field called ``BERTology''. In a paper titled ``A Primer in BERTology: What We Know About How BERT Works'' \cite{rogers2020primer}, a ``survey of over 150 studies of the popular BERT model'',  the authors explain:
\begin{quote}
Although it is clear that BERT works remarkably well, it is less clear why, which limits further hypothesis-driven improvement of the architecture. Unlike CNNs, the Transformers have little cognitive motivation, and the size of these models limits our ability to experiment with pre-training and perform ablation studies. This explains a large number of studies over the past year that attempted to understand the reasons behind BERTâ€™s performance. In this paper, we provide an overview of what has been learned to date, highlighting the questions that are still unresolved.
\end{quote}
This is a strange situation. Engineers built something and then scientists created a science to understand it!  

Clearly there is a need for further study from connectionist and computational neuroscience standpoints (BERTologists are mainly computational linguists).  These networks develop complex internal representations using neural networks, which are brain like, and perform at amazingly human levels, as we'll now see.

% One shot learning etc.

% TODO: More on applications to cog-sci
% Ethics discussions? An old note: Some philosophical analysis of the promise and potential dangers of this technology are in \cite{floridi2020gpt}.