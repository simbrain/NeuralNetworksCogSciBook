\chapter{Least Mean Squares and Backprop}\label{ch_lms_backprop}
\chapterauthor{Jeff Yoshimi}

% Beautiful! https://xnought.github.io/backprop-explainer/
% High level idea of "Gradient flow" backwards, and how you can make things modular

Having introduced supervised learning in chapter \extref{ch_supervised}, in this chapter we consider several supervised learning algorithms for feedforward networks in depth, and consider their implications for cognitive science. First, an early form of supervised learning called the ``Least Mean Squares rule'' or ``LMS rule''. The remarkable thing about LMS is that almost all of the advancements in the area of neural networks in recent years arguably involve variations on this one simple algorithm, that allow it to solve increasingly complex problems. LMS itself is limited insofar as it can only be used to train networks with one weight layer. This takes us to a second algorithm, backpropagation or ``backprop'', which extends LMS in a way that allows it to be used to train many-layered networks.  The hidden layers of a feedforward network develop \emph{internal representations}, which remap the input space of a neural network in useful and often psychologically realistic ways.

\section{Least Mean Squares Rule}\label{lms_rule}

We now consider our first supervised learning algorithm in detail: the \glossary{Least Mean Squares} rule or ``LMS'' or ``Delta rule'', which uses a form of gradient descent to minimize the error on a training set. It makes a nice contrast with Hebbian learning. Although the rule itself is similar in form, it is \emph{adaptive}.  While repeated application of the Hebb rule leads weights to explode to maximum or minimum values, LMS follows the Goldilocks principle, so that the weights zoom in on target values until the output is ``just right.''\footnote{This rule is a descendent of Rosenblatt's perceptrons (\cite{rosenblatt1960perceptron}; also see chapter \extref{ch_history}), which had one weight layer and a single binary output that was applied to classification tasks.  His perceptron learning algorithm involved an if-then rule which would apply a form of gradient descent if an input was misclassified. Since both targets and outputs were thresholded binary units, this led to weight and bias updates that were ``jittery'' and sometimes unstable. The LMS rule, associated with Widrow and Hoff, was similar to the perceptron learning rule but worked with linear or sigmoidal outputs. (This is a natural progression insofar as a sigmoidal activation function can approximate a threshold activation function when the slope at the inflection point is steep enough). It was derived using calculus by finding the derivative of a point on an error surface over parameter space, and updating the parameters in a direction that led to error reduction (see section \extref{sect_gradient_descent}). It thus replaced the effort to create learning rules using intuition with a principled mathematical way of deriving learning rules mathematically. The same mathematical methods are still in use today and have enabled many of the incredible advances that have occurred in the field. Note that terminology  in this  area is not entirely consistent. For example, any network trained by LMS is often called a ``Perceptron''. }  
% If this improves enough consider promoting to main text

Note that LMS is an algorithm that only works with 2-layer networks. It cannot be directly applied to multi-layer networks. That is an important limitation that is overcome by backprop, as we will see.
% For linear networks, it does not matter how many layers they are, they are the same. 
% Any linearly separable problem can be solved with this algorithm. The perceptron convergence theorem.

LMS follows the template from section \extref{SupervisedFirstPass}: begin with random parameters (weights and biases), iterate through each of the input patterns, compute errors, and use these errors to update weights and biases in such a way that errors are reduced. We will focus on the case of a single linear output node here. \footnote{We will not formally derive it, or state the algorithm in its full generality. However, the derivation is not too difficult: the key step involves taking the derivative of the error function with respect to a weight (how much is error changing as a function of that particular weight). A brief derivation is at \url{https://en.wikipedia.org/wiki/Delta_rule} and a more detailed discussion is at \url{http://uni-obuda.hu/users/fuller.robert/delta.pdf}. In subsequent planned sections we derive the rule, and extend it rule to handle many outputs, and more complex activation functions.} When applying the rule, a change in a weight $w_{i,j}$ is equal to the product of a learning rate $\epsilon$, the activation of the input node to that weight, $a_i$, and the difference between a desired and actual activation ($t_j - a_j$) for that node:
\begin{eqnarray*}
\Delta w_{i,j}  =  \epsilon a_i (t_j - a_j)
\end{eqnarray*}
Since $(t_j - a_j)$ is error  (with a small ``e''), the rule says that the change in a weight is equal to a learning rate times the activation of the input node for that weight, times the error (that is, $\Delta w_{i,j}  =  \epsilon a_i e_j$).  This means that for positive inputs, weights are changed in the direction of error.\footnote{The input scales things so they work for positive or negative inputs and so that changes are bigger for bigger inputs, and the learning rate allows us to control how quickly learning happens. Even though we are giving interpretations for each factor in the rule, it can be derived purely from calculus.} Notice that the form of the rule is identical to the Hebb rule, except with output activation replaced by error. 

LMS changes the bias $b_j$ of an output node $j$ as follows: 
\begin{eqnarray*}
\Delta b_j  =  \epsilon (t_j - a_j)
\end{eqnarray*}
That is, the change in a bias for a node $j$ is just the learning rate times the error on output node $j$.\footnote{Note that this is really the same as the weight change rule, if we think of the bias in terms of another input neuron, clamped at 1, and attached to this output neuron by a modifiable weight (which is in effect the bias).}

% The new_material doc contains additional material here, but I'm not sure it's useful. 
% Include new teaching method from "teaching challenges" doc

The LMS rule directly instantiates the Goldilocks principle. When heating up soup, or a bath, or a room, we adjust up and down based on errors.\footnote{In fact, the LMS rule is comparable to a bunch of thermostats, one for each output node of a network, where the targets correspond to the settings on the thermostats.} Oh, it's too cold, let's heat it up. Oh wait, now it's too hot, cool it down. Ok now it's perfect, stop changing things, it's just right. In a similar way here we increase and decrease parameters until error is as low as possible. Consider how this works in a few cases where we assume a positive input:
\begin{itemize}
\item Target is 2 and output is 1. Output is too low. Error is $2-1 = 1$ and so we make the weight stronger.
\item Target is 1 and output is 2. Output is too high. Error is $1-2 = -1$ and so we make the weight weaker.
\item Target is 1 and output is 1. Output is just right. Error is $1-1 = 0$ and so we don't change a thing.
\end{itemize}
% That is, the error tells us what direction we need to change the weight, when inputs are positive. The source activation tells us by how much. It's too hot, error is negative, it needs to be cooler. It's too cold, error is positive, it needs to be warmer. It's just right, error is 0, don't change it at all.

% This material is useful but not ready for prime time yet
%We also end up multiplying by the source value? Why? Well, think about it, again in the two cases.
%\begin{enumerate}
%\item Large source activation. This is making a big change in the output. So we want any change to the weight and bias to be proportional to the source activation. The output was too high, and this source activation is large, so we want to make the change in the weight large, to compensate for the large activation
%\item Small source activation. The output was too high, but this source activation was not high, so we just change things a bit. It's contributing less to the problem.
%\end{enumerate}
%Also, negative source activations will make the weight change change sign as well, so that we make changes in the right direction. % TODO: Examples

%This is like a variant on the Hebb rule. Hebb said, just change the weight based on the product of source and target. Neurons that fire together wire together. That has use for detecting patterns of correlation between input and output. Make the output reflect those correlations. But now we say something different. We change weight based on the product of source activation and the error. We in a sense correlate with the error, which reduces the error, until the error is close to 0.
%
%One last way to think of this is in terms of statistical concepts. We can think of LMS as repeatedly testing the network and correcting for false positives (the output is too high; a ``type 1'' error) and false negatives (the output is too low; a ``type 2'' error) until the error rate is as low as possible. This analysis makes mmost sense for classification tasks with binary variables.

% Discuss in a footnote (here  or above) convex optimization and how there is always a global minimum.
% https://towardsdatascience.com/understanding-convexity-why-gradient-descent-works-for-linear-regression-aaf763308708

\section{LMS Example}\label{lms_example}

In this example and in subsequent practice questions, assume we have a  simple 1-1 feed-forward network  (as in the left panel of figure \extref{tables_nets}), and that the slope of the output node is 1 and bias is 0. So we have two nodes, with activations $a_1$ and $a_2$, and a weight $w_{1,2}$. We also assume a very simple labeled dataset with a single row: one input value and one corresponding target value. That is:
\begin{center}
\begin{tabular}{| c || c | }
\cline{1-2}
\multicolumn{1}{| c || }{inputs}
 & \multicolumn{1}{c|}{targets} \\
\hline
  1 & 2  \\
\hline
\end{tabular}
\end{center}
We label the target value $t$. We will not consider updates to the bias term. As in the discussion of the Hebb rule (chapter \extref{ch_unsupervised}), we use the prime symbol $'$ to indicate a variable after a delta term has been applied.

% Explicitly explain that a prime means next time step, for all variables
We can now work out a complete example, and in the process see how LMS implements gradient descent. Suppose we are given:
\begin{eqnarray*}
& a_1 = 1 \\
& w_{1,2} = 1.5 \\
& t = 2  \\
& \epsilon = .5  \\
\end{eqnarray*}
With this information we can determine: (1) the output activation $a_2$ of the network (our ``forward pass''), (2) SSE, (3) the updated weight value at the next time step, which we designate $w_{1,2}'$, (4) the output activation $a_2'$, and (5) error at the next time step, SSE$'$. We can then repeat these steps and check to see that SSE is reduced over time, which moves us down the error surface for this task, which is shown in Fig. \ref{error_lms}.

(1) The network will produce a $1.5$ in response to an input of $1$, since the input activation is $1$, the weight is $1.5$, and $1 \cdot 1.5 = 1.5$ (we are, again, assuming output bias of 0 and slope of 1). 

(2) SSE for these simple networks and labeled datasets is very easy, since there is just one row, one target value, and one output value. That is,  SSE $= (t - a_2)^2$ or in this case $(2-1.5)^2 = .5^2 = .25$. Notice that this puts us at the point $(1.5,.25)$ in the graph in Fig. \ref{error_lms}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/error_lms.png}
\caption[Jeff Yoshimi.]{Gradient descent on the error surface for the LMS example discussed in Section \ref{lms_example}. As the LMS rule is applied the weight strength changes in a way that minimizes sum squared error.}
\label{error_lms}
\end{figure}

(3) Applying the formula above
\begin{eqnarray*}
\Delta w_{i,j}  =  \epsilon a_i (t_j - a_j)
\end{eqnarray*}
we get
\begin{eqnarray*}
\Delta w_{1,2}  =  .5 \cdot 1 \cdot (2- 1.5) = .25
\end{eqnarray*}
We then use $\Delta w$ to update the weight value from its old value of 1.5, so that
\begin{eqnarray*}
w_{1,2}' = w_{1,2} + \Delta w_{1,2}  = 1.5 +.25  = 1.75
\end{eqnarray*}
So our new weight value is 1.75. 

(4) With this new weight, the network produces an output  $a_2' = 1 \cdot 1.75 = 1.75$. 

(5) The squared error is now $(2-1.75)^2=.25^2=.0625$, whereas before it was $.25$. So we have moved to the point $(1.75,.0625)$ in the graph  of the error surface in Fig. \ref{error_lms}. An improvement!  We have moved lower on the error curve; we have descended the error gradient.

In subsequent time steps we get:

\begin{eqnarray*}
w_{1,2}'' = 1.75 + .5 \cdot 1 \cdot (2 - 1.75) = 1.875 \\
w_{1,2}''' = 1.875 + .5 \cdot 1 \cdot (2 - 1.875) = 1. 9375 \\
\end{eqnarray*}
As you can see, applying this rule leads to SSE getting lower and lower, and the output getting closer and closer to the desired output of 2. Ten successive points on the error curve are shown in figure \ref{error_lms}.

\section{Linearly Separable and Inseparable Problems}
\label{linearlySeparable}

% glossary
% Add "donut" and other decision problem situations. Refer to sklearn page.

Two-layer feed-forward networks with linear output nodes, like LMS networks, are in a certain way limited. That limitation played  an important role in the history of neural networks, paving the way for studies of \emph{internal representations} in neural networks, which had lasting consequences both in machine learning and in connectionist applications of neural networks to psychology.  The limitation concerns the \glossary{linearly separable} classification tasks. Thus, in this section, and in much of the rest of the chapter, we focus on classification rather than regression.

% Do this with pictures for the classes, or even the inputs (see the wikipedia picture referenced above).
% Decision region and boundary should refer back to last chapter where there should be glossary entries
To understand what linear separability (and inseparability) are, recall that a classification task assigns each input to a different category. If we focus on networks with two input nodes and one output node, then we can plot a classification task as in figure \extref{visualize_classification} (Right), but we can also directly label the points as 0 and 1, as in figure \ref{F:decisionBoundaries}. When we create this type of plot, it often becomes immediately clear what the relationship between the categories is, in the input space. In figure \ref{F:decisionBoundaries} (Left), for example, we can immediately see that the two classes are distinct in the input space. Notice that we can separate the two categories by drawing a line between them, as in figure \ref{F:decisionBoundaries} (Middle). Such a line is, as we saw in section \extref{visClassification}, a \emph{decision boundary}, which has the effect of separating the input space in to two \emph{decision regions}, one for each possible classification. Input vectors in the region below the decision boundary will be classified as 0, while those in the region above the boundary will be classified as 1. However, note that for the task shown in figure \ref{F:decisionBoundaries} (Right) there is no way to use a line to separate the 0's and 1's perfectly.
% Use example? Each input vector will either be classified as in the class (1) or not in the class (0). For example, if the inputs were features of milk samples, then spoiled milk might produce an output of 0, good milk 1  

\begin{figure}[h]
\centering
\raisebox{-0.5\height}{\includegraphics[scale=.4]{./images/decisionBoundaries1.png}}
\hspace*{.4in}
\raisebox{-0.5\height}{\includegraphics[scale=.4]{./images/decisionBoundaries2.png}}
\hspace*{.4in}
\raisebox{-0.5\height}{\includegraphics[scale=.4]{./images/decisionBoundaries3.png}}
\caption[Jeff Yoshimi.]{Three classification tasks. (Left) A linearly separable task. (Middle) A decision boundary that will solve the task. (Right) A linearly inseparable task and a non-linear decision boundary that can solve it.}
\label{F:decisionBoundaries}
\end{figure}
% Cohere this better with the section on tables and training tasks

If a classification task can be solved using a decision boundary which is a line (or, in more than 2-dimensions, a plane or hyperplane), the classification problem is called a \glossary{linearly separable} problem. Figure \ref{F:decisionBoundaries} (Middle) shows a linearly separable problem. When we cannot properly separate the categories with a line (or hyperplane), as in figure \ref{F:decisionBoundaries} (Right), the problem is \glossary{linearly inseparable}, there is no way to draw a line which separates the 0's and 1's in that example. There is no linear decision boundary for that problem (though there are non-linear decision boundaries that perfectly separate the classes, like the wavy curve shown in the figure).\footnote{First, note that in the case shown, we could still fit a line to the problem and we'd just have some error. Second, we will see that while LMS cannot solve this task, since it uses linear decision boundaries, other supervised classification algorithms like backprop exist that can solve these types of non-linearly separable classification task}

The goal of supervised learning of classification tasks is to set the weights of a network so that the decision boundary properly separates the two classes. The values of the weights and the output bias are like knobs that, when turned, will change where the decision boundary is: it can be rotated around and moved up and down. We want to turn the knobs so that the two classes are properly separated.\footnote{Doing this amounts to minimizing error. When the decision boundary properly separates the two classes, SSE will be 0. If not, as in Fig. \ref{F:decisionBoundaries} (Right), there will be some error. What will SEE be in that case?}  However, LMS only allows linear decision boundaries. Other algorithms have more knobs, and can be used to create more complex decision boundaries and decision regions.
% Footnote on more complex decision regions here, or somewhere else? And the interesting material on how with more layers we get more complex regions. See the code comments at the end of this section. Also see the videos on this, which contain useful summary. References to Lipman and to Bishop chapter 4, e.g. figure 4. I feel I have written this up before so I may have it in notes.
  
Logic gates provide a convenient and historically important class of tasks that can be used to further illustrate these ideas. In appendix \extref{ch_logicgates} it is shown that logic gates can be represented as 2-1 feed-forward neural networks. Pairs of input nodes corresponding to statements P and Q connect to output nodes representing boolean combinations of truth values (0 for false, and 1 for true): P AND Q (true when both are true), P OR Q (true when at least one is true), and P XOR Q (true only when one is true). These can be depicted using the same kinds of classification plots as above (here using open dots for 0, and filled dots for 1). Fig. \ref{boolean_inputspace} shows the input space for the AND, OR and XOR logic gates. Note that AND and OR are linearly separable, and that XOR is not.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{images/booleanInputSpaces.png}
\caption[Jeff Yoshimi.]{Input spaces for AND (left), OR (middle), and XOR (right). Open dots correspond to 0, filled dots to 1. Which tasks are linearly separable?}
\label{boolean_inputspace}
\end{figure}

Now we get to the major problem affecting two layer networks trained using LMS: \emph{they cannot solve linearly inseparable classification tasks}, like XOR. That two-layer linear networks cannot solve these problems was a major issue in the early history of connectionism. In 1969 Marvin Minsky and Seymour Papert published a book called {\em Perceptrons} (perceptrons were 2-weight-layer networks Rosenblatt trained; see section \extref{ageOfPerceptron}). In this book Minsky and Papert showed that such networks could not solve linearly inseparable problems \cite{minsky1969perceptrons}. This had a disastrous impact on neural network research in the following decade, during the ``dark ages'' of neural networks (see section \extref{dark_ages}). As Rumelhart and McClelland recall:

\begin{quote}
Minsky and Papert's analysis of the limitations of the one-layer perceptron\footnote{They are referring to a single weight layer connecting two layers of nodes. So what Rumelhart and McClelland call a ``one-layer'' network is what we have called a ``2-layer'' network}, coupled with some of the early successes of the symbolic processing approach in artificial intelligence, was enough to suggest to a large number of workers in the field that there was no future in perceptron-like computational devices for artificial intelligence and cognitive psychology (PDP 1, p. 112) \cite{rumelhart1986parallel}.
\end{quote}

However, as Rumelhart and McClelland go on to point out, these results don't apply to neural networks with more than 2 layers \cite{rumelhart1986parallel}. In fact, it's not too hard to solve XOR by hand, just by combining an or gate with an intermediate and gate (see appendix \extref{ch_logicgates}). The key point, that we will keep coming back to, is that intermediate representations allow complicated and linearly inseparable problems like XOR to be solved.\footnote{In fact, it has since been shown that multilayer neural networks with sigmoidal activation functions in the hidden layers are universal approximators in the sense that they can, in principle, approximate almost any vector-valued function (more specifically, any ``Borel measurable function from one finite-dimensional space to another'' \cite{hornik1989multilayer}. This has come to be known as the universal approximation theorem, and there is now a detailed Wikipedia page on the topic: \url{https://en.wikipedia.org/wiki/Universal_approximation_theorem}.}

So multi-layer feed-forward networks can solve linearly inseparable problems. Great!  But alas, there was another problem. Initially there was no way to train multi-layered networks. LMS only works on 2 layer networks. Sure we can hand-craft solutions in particular cases (like the combined and and or gates approach to XOR), but can we make the process automated? Initially the answer was no (HISTORY REF), but later it was yes.\footnote{Minsky and Papert, who first clearly identified this problem, recognized that adding hidden layers could surmount the limitations they described. However, they thought that multi-layer networks were {\em too} powerful, describing them as ``sufficiently unrestricted as to be vacuous'' (Rumelhart and McClelland, p. 112) \cite{rumelhart1986parallel}. In particular, Minsky and Papert pointed out that no one knew how such a network could be trained to solve specific pattern association tasks \cite{minsky1969perceptrons}.}

Once algorithms were discovered that could be used to train multi-layer networks, it became possible to have networks learn solutions to linearly inseparable classification tasks (by finding non-linear decision boundaries) and to deal with much more complex problems than had previously been possible. The most famous algorithm of this type was backprop, which we turn to now.

\section{Backprop}\label{backprop}

In this section we cover what is probably the best known form of supervised learning: \glossary{backpropagation} (or ``backprop''). Backpropagation is a powerful extension of the Least Mean Square  technique. As we saw, LMS only works for two-layer networks. Backprop works for a much broader class of networks, in particular networks with non-linear activation functions containing one or more hidden layers. As we will see, these hidden layers allow a network to transform inputs into different types of representation, and in doing so makes them quite powerful (almost all modern neural networks use variants of backprop), and also psychologically interesting. 

Backprop can be thought of as a generalization of the LMS technique or ``Delta rule'' described in the last few sections. In fact, backprop is sometimes called the ``generalized delta rule.'' This rule had been proposed as early as the late 1960s / early 1970s \cite{bryson1969applied, werbos1974beyond} and was independently discovered by several theorists in the 1980s \cite{le1986learning, parker1985learning}.\footnote{A detailed study of the history of backprop, with reference to others beyond those already cited, is \cite{schmidhuber2020deep}, available at \url{https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html}.} It was popularized by Rumelhart, McClelland, and Williams in the late 1980s \cite{rumelhart1986parallel}. The discovery and popularization of backprop led to a revival of interest in neural networks in the 1980s and 1990s, following the ``dark ages'' of the 1970s (again, see chapter \extref{ch_history}).

We will not cover the details of the backpropagation algorithm here\footnote{Though, as with the details LMS, a detailed discussion of backprop is being planned now.}, but will instead describe it in a qualitative way. In fact, it is identical to LMS at the output layer, but adds a way to update the hidden weights. In fact, the weight updates are also almost the same, but the computation of the error is more complex, because we don't have direct access to the targets. That is, for a weight $w_{i,j}$ connecting an input node $a_i$ to a hidden layer node $a_j$, the update rule is:
\begin{eqnarray*}
\Delta w_{i,j}  =  \epsilon a_i e_j
\end{eqnarray*}
Where $e_j$ is now just a different kind of error. This error is computed by a kind of reversal of how weighted inputs are computed in forward propagation. When weighted inputs or net inputs (see chapter \extref{ch_act_functions}) to a node are computed, activations are multiplied by intervening weights and added together. In linear algebra terms, the dot product of an input vector and fan-in weight vector is taken. This propagates activation forward. But now, we do the reverse. We take the errors on nodes at some level of a feedforward network, and multiply them by the intervening weights from the previous layer, and add the resulting products.  That is, we backpropagate error. The idea is illustrated in figure \ref{backprop_error}. In the example shown, assume the weights from the hidden unit to the output are 1, so that the backpropped error is $2 * 1 + 0 * 1 = 2$. That error can then be used to update $w_{1,2}$ using  $\Delta w_{1,2}  =  \epsilon a_1 e_2$. Notice that all that has changed from LMS is that we got the error via this backpropagation procedure.\footnote{If $\epsilon = 1$, the weight  $w_{1,2}$ will be updated from 1 to $1 + 1 \cdot 1 \cdot 2 = 3$. This will improve $a_3$ but make $a_4$ worse (however, repeated application of the rule with a lower learning rate $\epsilon$  would reduce error).}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/backpropError}
\caption[Jeff Yoshimi.]{How error is backpropagated. Output errors are multiplied by intervening weights and added together. In this example, assume the target values are $t_3 = 3$ and $t_4 = -1$ and the weights are all $1$. The errors at the output layer are $e_3 = 3-1=2$ and $e_4 = -1 - (-1) =  0$.  These errors are multiplied by the intervening weights to the hidden unit and added together (the errors are ``backpropagated'') to get a new error $e_2 = 2 * 1 + 0 * 1 = 2$ at the hidden unit shown.}
\label{backprop_error}
\end{figure}
% Maybe another example with other weight values

Intuitively, this corresponds to how much a  hidden unit contributes to an output nodes' error. It's as if ``blame is assigned'', and on this basis the weights to the hidden layer are updated. The rule can be applied recursively, starting at the final output layer of a feedforward network and moving backwards through all previous layers. Thus it can be used to train many-layered networks. 

As with LMS, backprop works by minimizing an error function with respect to a training set, so that we have gradient descent on an error surface. However, since multi-layer feed-forward networks are more complex than 2-layer networks, the error surface is more complicated. With two-layer linear networks, the error surface has a relatively simple bowl-like structure, which often has a single minimum value at the low-point of a ``bowl'' shape. With a multi-layer non-linear network, the error surface can be more complex and wavy, and there can be multiple local minima (compare figure \extref{local_minima}). These local minima can ``trap'' the gradient descent procedure, producing sub-optimal solutions.

One simple way to try to deal with the issue is by re-initializing the parameters and re-running the algorithm. Each time this is done, the system moves to a different point on the error surface and tries to find the local minimum from that spot. By trying multiple times one can ``search'' for the lowest minimum possible.\footnote{Compare the way we searched for fixed points in chapter \extref{ch_dst}, by starting at different random points in state space.}  This is like dropping a marble at different spots on the error surface and comparing how low the marble goes each time. In this way we can find the lowest of several local minima (which might turn out to be the global minimum) and in this way we can try to improve a network's performance on a task. In practice, however, one uses advanced optimization techniques that do things to automatically search for a global minimum.\footnote{Currently the industry standard seems to be the ``Adam'' method: \url{https://arxiv.org/pdf/1412.6980.pdf}.}

\section{XOR and Internal Representations}\label{sect_xor_remap}

% Deep learning mention?  Ref. to cortex discussion?
We have not described in detail how the backprop algorithm works. However, we can get insight into what it does by considering what happens in the hidden layer of a network trained using backprop. In particular, we can begin to understand how multi-layer networks, like our brains, can solve problems by remapping input spaces to hidden unit spaces that contain useful internal representations. 

The classic example to illustrate these ideas is the XOR problem. Recall that XOR, considered as a vector valued function, is not linearly separable (figure \ref{boolean_inputspace}, right). Here is the labeled dataset we would use to train a network to implement XOR:
\begin{center}
\begin{tabular}{| c | c || c | }
\cline{1-3}
\multicolumn{2}{| c || }{inputs}
 & \multicolumn{1}{c|}{targets} \\
\hline
  $x_1$  & $x_2$ & $t_1$  \\
\hline
  0 & 0 & 0  \\
\hline
 1 & 0 & 1  \\
\hline
 0 & 1 & 1 \\
\hline
1 & 1 & 0 \\
\hline
\end{tabular}
\end{center}
As we saw, two-layer networks with linear units cannot solve this type of problem, but a three-layer network with non-linear units can solve it. This is easy to confirm in Simbrain: try training an LMS and Backprop network on this data, and notice the difference in the minimum error you can achieve in the two cases. % Check and report these numbers?  
	
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{images/xor_internal_rep.png}
\caption[Pamela Payne.]{A remapping of the input space to the hidden unit space in the XOR problem. Note that the bottom panel shows the input space for XOR, and that it is linearly inseparable. The network then maps $(0,0)$ and $(1,1)$,  to $(0,0)$ in the hidden unit space. $(0,1)$ and $(1,0)$ are mapped to $(0,.5)$ in the hidden unit space. Now notice that the hidden unit space is linearly separable!  Also notice that the hidden unit space has developed an internal representation of the two main cases of interest: just one unit is one (represented by $(0,.5)$) and both units are in the same state (represented by $(0,0)$). Thus the separated hidden unit states can be mapped to the appropriate output states.}
\label{xor_remapping}
\end{figure}


The key to backprop's superior performance is the way it \emph{re-maps} the linearly inseparable problem in the input space to a linearly separable problem in the hidden unit space, as shown in figure \ref{xor_remapping}. The crucial thing the hidden layer did was transform the input layer representation into a new internal representation, which includes a representation of ``only one unit is on'' and another representation of ``both units are in the same state.''  These two states are now linearly separable, and the output layer can easily separate them. The solution shown in the figure was produced by training a 2-2-1 network using backprop. Other solutions (corresponding to other minima in the error surface) can also be found. You are encouraged to try the experiment yourself in Simbrain. Train a backprop network on XOR, get it to a minimum on the error surface, and then check to see what hidden layer activations occur for each input.

\newpage

\section{LMS Exercises}

% If more questions are added, the name of the section can change or new sections can be added

For all questions assume output nodes have a linear activation function (no clipping) and bias = 0.

\newcounter{LMSCounter}

\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 1, t_2 = 4, a_2 = 2, \epsilon = 1$, what are $e_2$ and $\Delta w_{1,2}$? \\
{\bf Answer:}  \\
(1) $e_2 = t_2 - a_2 = 4 - 2  = 2 $ \\
(2) $\Delta w_{1,2} = \epsilon a_1 e_2 = 1 \cdot 1 \cdot  2 = 2$
\bigskip

\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 1, t_2 = 1, a_2 = 2, \epsilon = 1$, what are $e_2$ and $\Delta w_{1,2}$? \\
{\bf Answer:}  \\
(1) $e_2 = 1 - 2  = -1 $ \\
(2) $\Delta w_{1,2} = \epsilon a_1 e_2 = 1 \cdot 1 \cdot  -1 = -1$
\bigskip

\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 1, t_2 = 2, a_2 = 2, \epsilon = 1$, what are $e_2$ and $\Delta w_{1,2}$? \\
{\bf Answer:}  \\
(1) $e_2 = 2 - 2  = 0 $ \\
(2) $\Delta w_{1,2} = \epsilon a_1 e_2 = 1 \cdot 1 \cdot 0 = 0$
\bigskip

\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 1, t_2 = 1, a_2 = 9, \epsilon = .25$, what are $e_2$ and $\Delta w_{1,2}$? \\
{\bf Answer:}  \\
(1) $e_2 = t_2 - a_2 = 1 - 9 = -8 $ \\
(2) $\Delta w_{1,2} = \epsilon a_1 e_2 =  .25 \cdot 1 \cdot (-8)  = -2$
\bigskip


% TODO: Bias

% Combined questions


\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 1, w_{1,2} = 1, t_2 = 2,\epsilon = 1$, what are the initial activation $a_2$, error $e_2$, and $\Delta w_{1,2}$, and after one weight update, the updated weight $w_{1,2}'$, updated activation  $a_2'$ and updated error $e_2'$? \\
{\bf Answer:}  \\
(1) $a_2 = a_1 \cdot w_{1,2} = 1 \cdot 1 = 1$ \\
(2) $e_2 = t_2 - a_2 = 2 - 1 = 1 $ \\
(3) $\Delta w_{1,2} = \epsilon a_1 e_2 = 1 \cdot 1 \cdot 1 = 1$ \\
(4) $w_{1,2}' = w_{1,2} + \Delta w_{1,2}  = 1 + 1 = 2$\\
(5) $a_2' =  a_1 \cdot w_{1,2}' = 1 \cdot 2 = 2$\\
(6) $e_2' = (2-2) = 0$
\bigskip

\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 2, w_{1,2} = 1, t_2 = 3 ,\epsilon = .1$, what are the initial activation $a_2$, error $e_2$, and $\Delta w_{1,2}$, and after one weight update, the updated weight $w_{1,2}'$, updated activation  $a_2'$ and updated error $e_2'$? \\
{\bf Answer:}  \\
(1) $a_2 = a_1 \cdot w_{1,2} = 2 \cdot 1 = 2$ \\
(2) $e_2 = t_2 - a_2 = 3 - 2 = 1 $ \\
(3) $\Delta w_{1,2} = \epsilon a_1 e_2 = .1 \cdot 2 \cdot 1 = .2$ \\
(4) $w_{1,2}' = w_{1,2} + \Delta w_{1,2}  = 1 + .2 = 1.2$\\
(5) $a_2' =  a_1 \cdot w_{1,2}' = 2 \cdot 1.2 = 2.4$\\
(6) $e_2' = (3 - 2.4) = .6$
\bigskip

\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 1, w_{1,2} = 3, t_2 = 1 ,\epsilon = 1$, what are the initial and final error $e_2$ and $e_2'$ after one weight update? 
{\bf Answer:}  \\
(1) $e_2 =  1 - 3  = -2 $ \\
(2) $e_2' = 1 - 1 = 0$
\bigskip

\noindent
\stepcounter{LMSCounter}
{\bf \theLMSCounter.}  Given $a_1 = 1, w_{1,2} = -1, t_2 = 1 ,\epsilon = 1$, what are the initial and final error $e_2$ and $e_2'$ after one weight update? 
{\bf Answer:}  \\
(1) $e_2 =  1 - (-1)  = 2 $ \\
(2) $e_2' = 1 - 1 = 0$
\bigskip


% Add some questions involving multiple inputs, and changing biases
