\chapter{New Material LMS and Backprop}
\chapterauthor{Jeff Yoshimi}

\subsection{Derivation of the LMS rule}

The nice thing about LMS is we can directly interpret it in terms of Goldilocks.

All this can be shown using calculus. It's kind of amazing to turn the crank on the calculus, and see how it all works out giving us the rule above, with all the intuitive meaning. Run through the calculus, and we get our rule. 

% Possibly just drop most of the i,j subscripts in this section since it's clearly a 2 node network. 
Conceptually, we are computing the derivative of the error (let us suppose we are taking the sum of squared errors or SSE; that is $E = SSE$) with respect to the parameters. Assuming a training set specifying input and targets is given. Our error E is then a scalar value that varies with the parameters, the weights and biases. Since we have many parameters, the derivative of E with respect to all of them is a \emph{gradient} vector, an ordered list of partial derivatives of E with respect to each of $p_1,\dots, p_n$.  This is a vector that points in a direction tangent to the error surface, in a direction where changing the parameters has the biggest impact on the rate of change of error.  However, here we will assume the bias is fixed and only consider a single weight from an input to an output, to make things easier, and a training set with just one input-output pair.  In this case, error is just the squared difference between the target value and the output value relative to the input:

\begin{eqnarray*}
E = (t_2 - a_2)^2  =  (t_2 - ( a_1 w_{1,2} + b_2))^2  =  (t_2 -  a_1 w_{1,2} -  b_2)^2 
\end{eqnarray*}
In this case the gradient reduces to a simple derivative of $E$ with respect to a weight $w_{1,2}$.  Using the chain rule (derivative of the outside part applied to the inside part, times the derivative of the inside part), and substituting, we get

\begin{eqnarray*}
\frac{dE}{dw_{1,2}} = 2 (t_2 - a_1 w_{1,2} - b_2) (-a_1) =  -2 a_1 (t_2 - a_1 w_{1,2} - b_2) =  -2 a_1 (t_2 - a_2)
\end{eqnarray*}

And since $t_2 -  a_2$ is error $e_2$ we get

\begin{eqnarray*}
\frac{dE}{dw_{1,2}} = -2 e_2 a_1
\end{eqnarray*}

This says that the change in error with respect to the weight is proportional to the product of the error and the source activation. This motivates a rule where we adjust the weight proportionally to this derivative, using a learning rate. The 2 can be ignored, since we can think of it as being absorbed into the learning rate. Also, this gradient points in the direction of error increasing. But we want to go in the opposite direction, so that error is reduced (this is gradient descent, not gradient ascent). So we multiply the gradient by $-1$, changing its sign. And so our learning rule is 

\begin{eqnarray*}
\Delta w_{1,2}  =  \epsilon a_1 e_2  = \epsilon a_1 (t_2 - a_2)
\end{eqnarray*} 

Of course this generalizes, and so our general learning rule for weights is:
\begin{eqnarray*}
\Delta w_{i,j}  =  \epsilon a_i (t_j - a_j)
\end{eqnarray*} 

The rule for bias can be derived in a similar way. Again the error function is

\begin{eqnarray*}
E =  (t_2 - ( a_1 w_{1,2} + b_2))^2 
\end{eqnarray*}
but in this case we want to find $\frac{dE}{db_{2}}$. It is easy to check that the derivative is $-2(t_2 - a_2)$. As before, the negative sign means we go in the opposite direction for gradient descent, so after flipping the sign and absorbing the 2 into the learning rat, the bias update rule can be expressed $\Delta b_j = \epsilon (t_j - a_j)$. This follows the same Goldilocks principle as the weight rule: if the output is too low (target > actual), the bias increases; if the output is too high, the bias decreases.

Now let's add in the activation function $f$ and derive the weight-update rule. Now the error function is:

\begin{eqnarray*}
E =  (t_2 - f(a_1w_{1,2} + b_2))^2   
\end{eqnarray*}

and so

\begin{eqnarray*}
\frac{dE}{dw_{1,2}} = 2 (t_2 - f(a_1w_{1,2} + b_2)) f'(a_1w_{1,2} + b_2) a_1 
\end{eqnarray*}

Substituting in $a_2 = f(a_1w_{1,2} + b_2)$, and recalling $n_2 = a_1w_{1,2} + b_2$  and $t_2 - a_2 = e_2$ 

\begin{eqnarray*}
\frac{dE}{dw_{1,2}}  = 2 e_2 f'(n_2) a_1 
\end{eqnarray*}

So the learning rule for weights is:
\begin{eqnarray*}
\Delta w_{i,j}  =  \epsilon a_i e_j f'(n_j)
\end{eqnarray*}

And similarly, the learning rule for biases is:
\begin{eqnarray*}
\Delta b_{j}  =  \epsilon e_j f'(n_j)
\end{eqnarray*} 

This is the same as before, but multiplied by the derivative of the activation function applied to the net input (the weighted input to the node). We discuss this more below. 

Note that the derivative of the activation function $f'$ is like a free parameter in this formula. Whatever your activation function $f$ is, to use gradient descent we just plug in $f'$ and we can do gradient descent. Situations like this allow backprop to be automated in a sense. But note that some derivatives have problems, like being undefined or 0, which effectively turns off update.

Intuitively, the idea is to scale the rate of weight change by how much the activation function is changing things. So when the activation function is changing things a lot, we want the weight to change a lot too. When it changes things very little, the derivative is small and so we change the weight less. It doesn't matter as much. Also note that if the derivative is 0, then learning stops. That can be a problem with piecewise linear and relu activation functions, and is why the threshold activation function can't be used at all. Again see below.

As a transition to the next section, we can express this rule in terms of movement in parameter space in a direction away from the direction of steepest ascent.  This is a concise way to express what gradient descent is using notation from vector calculus. But it need not be intimidating, it's just a way to arrange the symbols. The gradient vector is a vector of partial derivatives of the error (or loss) with respect to the parameters. The gradient $\nabla L$ of the loss or error function is a multi-dimensional derivative of the error with respect to a vector containing all the weights and biases. It points in the direction in the parameter space where changes will make error go up the most.  So gradient descent basically moves that vector in the opposite direction by an amount given by the learning rate $\epsilon$,  $-\;\epsilon\,\nabla L$. That is, start with a vector (potentially a massive one, in a modern neural network) of parameters:
\begin{eqnarray*}
  \mathbf{w} = \bigl(w_1, w_2, \dots, w_n\bigr).
\end{eqnarray*}
Then the gradient of our scalar loss \(L(\mathbf{w})\) is
\begin{eqnarray*}
  \nabla L(\mathbf{w})
    = \begin{pmatrix}
        \dfrac{\partial L}{\partial w_1}, 
        \dfrac{\partial L}{\partial w_2}, 
        \dots, 
        \dfrac{\partial L}{\partial w_n}
      \end{pmatrix}^\mathsf{T}.
\end{eqnarray*}
Gradient descent then updates all parameters at once via
\begin{eqnarray*}
  \mathbf{w} \;\leftarrow\; \mathbf{w} \;-\;\epsilon\,\nabla L(\mathbf{w}).
\end{eqnarray*}
In practice, we separately compute all these partial derivatives and just apply them separately to each weight and bias. But more abstractly, the whole collection of them ``points'' in some direction in the parameter space, and we are following that directional vector to find a way to go down to where error is lower.

\section{Vector approach to LMS}

In the scalar approach to LMS we multiply a single output error (target - output) by source activation. Now we have a vector of errors times a vector of source activations. But how to deal with indexing?  The algorithm is ultimately a variant on Hebb, so we already have a template for thinking about it (see the discussion of the vectorized Hebb rule)
% Reference the new material on Hebb here

The error part itself is easy, first we do target - output using elementwise subtraction: $\mathbf{t} - \mathbf{a}_{\text{out}}$. To multiply these errors times source activations, we do the same thing as in Hebb: we multiply an $m \times  1$ column output errors times a $1 \times  n$ row of source activations to get an $m \times n$ matrix of weight deltas. Then we scale that by learning rate, and add to the weight matrix. (There remains the issue of the bias, and any activation functions on the output, but we're starting simple.) So, the vectorized rule is
\begin{eqnarray}\label{vectorizedLMS}
\Delta \mathbf{W}  =  \epsilon (\mathbf{t} - \mathbf{a}_{\text{out}}) \mathbf{a}_{\text{in}}^T
\end{eqnarray}
Remember, by default all vectors in bold face are column vectors. So this is a column vector (the error $\mathbf{t} - \mathbf{a}_{\text{out}}$) times a row vector (the input activations) which produces a matrix in the same shape as the weight matrix.
% Integrate outer product here? If so see ency of scott "inner and outer product". Also see the discussion of outer product below and maybe move that here or repeat

Consider the network shown in figure \ref{lms_vector_pre}.

\begin{figure}[h]
\centering
\includegraphics[width=0.55\textwidth]{images/vectorLMSBeforeTrain.png}
\caption[Jeff Yoshimi.]{A network with empty weights. We want it to learn to associate the input vector $(0,1)$ with the output vector $(1,0,1)$, but right now the weights are all zero so it just outputs $(0,0,0)$. }
\label{lms_vector_pre}
\end{figure}

Remember, the weight matrix is a 3x2 matrix  (``target-source''), which we are treating as all zero in this example. Each row is a fan-in of an output node, and the columns are fan-outs of input nodes. Our template for thinking about this is to imagine inputs coming up from the bottom, and dotting with the fan-in weight vectors to produce outputs.

Our values are
\begin{equation*}
\epsilon = 1 \; \; \\
\mathbf{a}_{\text{in}} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \\
\mathbf{t} = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} \\
\mathbf{a}_{\text{out}} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
\end{equation*}

Then using equation \eqref{vectorizedLMS} (change in weight matrix is learning rate times error vector times input vector transposed), we get:

\begin{align*}
\Delta \mathbf{W}  = 1
\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} 
\begin{bmatrix} 0 & 1 \end{bmatrix} 
= \begin{bmatrix} 1 \cdot 0 & 1 \cdot 1 \\ 0 \cdot 0 & 0 \cdot 1 \\ 1 \cdot 0 & 1 \cdot 1 \end{bmatrix} 
= \begin{bmatrix} 0 & 1 \\ 0 & 0 \\  0 & 1  \end{bmatrix}
\end{align*}

So we get

\begin{align*}
\mathbf{W} + \Delta \mathbf{W}  =
\begin{bmatrix} 0 & 0 \\ 0 & 0 \\  0  & 0  \end{bmatrix} +
\begin{bmatrix} 0 & 1 \\ 0 & 0 \\  0  & 1  \end{bmatrix} =
\begin{bmatrix} 0 & 1 \\ 0 & 0 \\  0  & 1  \end{bmatrix}
\end{align*}

And again, keeping in mind that rows are fan-in of output and columns are fan-out of input we get figure \ref{lms_vector_2}.

\begin{figure}[h]
\centering
\includegraphics[width=0.55\textwidth]{images/vectorLMSAfterTrain.png}
\caption[Jeff Yoshimi.]{The network after training}
\label{lms_vector_2}
\end{figure}

We can now check that it works. Remember, think of the column vector on the right as ``entering'' from below, and dotting with each fan-in weight vector (each row of the matrix) to produce the output. SSE has gone to 0 in a single time step.

\begin{align*}
\begin{bmatrix} 0 & 1 \\ 0 & 0 \\  0  & 1  \end{bmatrix}
\begin{bmatrix} 0 \\ 1 \end{bmatrix}
= \begin{bmatrix} 1 \\ 0 \\ 1  \end{bmatrix}
\end{align*}

So now I suggest you try some examples of your own. Come up with your own training set, apply the rule and see if you can run this a few times. 

\subsection{Vectorized LMS with Bias and Activation Functions}

To incorporate bias updates and activation functions in the vectorized case, we use the Hadamard product ($\odot$) for elementwise multiplication:

\begin{eqnarray*}
\Delta \mathbf{W}  =  \epsilon ((\mathbf{t} - \mathbf{a}_{\text{out}}) \odot f'( \mathbf{n}_{\text{out}})) \mathbf{a}_{\text{in}}^T \\
\Delta \mathbf{b}  =  \epsilon (\mathbf{t} - \mathbf{a}_{\text{out}}) \odot f'( \mathbf{n}_{\text{out}})
\end{eqnarray*}

Where $\mathbf{n}_{\text{out}}$ is the vector of weighted inputs to the output layer, and $f'$ is applied elementwise. The intuition is the same as before: we scale our error corrections by how much the activation function is changing things. When the activation function is steep (large derivative), we make bigger weight changes. When it's flat (small derivative), we make smaller changes. For weights, we scale the error vector by the activation function derivatives, then take the outer product with input activations to get our weight update matrix. For biases, we simply scale the error vector by the derivatives—no need to consider input activations since biases don't depend on them.

\subsection{Batch version of the rule}

In many neural network implementations, computations are applied to batches of data. In this setup, inputs are not passed through a network as single column vectors, but whole batches are. This is more computationally efficient and leverages hardware optimization for matrix computations.  It thus helps to be able to think in terms of batches being processed.

By convention, batches are matrices where the rows are input vectors (or target vectors, output vectors, etc.), and so they have shape $B \times n$, where $B$ is the batch size and $n$ is the size of the input vectors. This is nice and easy to think about; basically the batches are little input tables (or output tables, etc; see section \extref{datasets}).

% Refer back to the AB = C discussion in linear algebra

For a network with two node layers and one weight layer $\mathbf{W}$, and with unbounded linear nodes, the forward pass is computed like this:

\begin{eqnarray}
\mathbf{Y} = \mathbf{X} \mathbf{W}^T
\end{eqnarray}
$\mathbf{X}$ is a  $B \times n_{in}$ batch of inputs, where $B$ is the batch size and $n_{in}$ is the number of nodes in the input layer. Again, it's a kind of input table. $\mathbf{W}$ is transposed so that we get a shape of $n_{in} \times n_{out}$. This is the source-target representation we emphasized in the discussion of linear algebra (see section \extref{sourceTarget}), where columns correspond to fan-in weight vectors. This allows us to think of the product as a bunch of vector-matrix multiplications (or ``right multiplications'' since the matrix is on the right), where we peel off the rows of the input table $\mathbf{X}$ one at a time, multiply them by the weight matrix (dotting the rows with each column of $\mathbf{W}^T$), in each case producing an output row vector in $\mathbf{Y}$. As we move down the rows of the input table we stack up rows of the output table or output batch $\mathbf{Y}$, which has a shape of $B \times n_{out}$.

To calculate error, we compare the output $\mathbf{Y}$ with a ``target table" (a batch of target values) $\mathbf{T}$. Subtracting the output activations from the target values  gives us an error matrix:
\begin{eqnarray}
\mathbf{E} = \mathbf{T} - \mathbf{Y}
\end{eqnarray}
where $\mathbf{E}$ also has the shape $B \times n_{out}$.

Now for backprop. The gradient of the loss with respect to the output activations in $\mathbf{Y}$ is proportional to the error matrix $\mathbf{E}$. If we are averaging the loss over the batch, this outer gradient becomes:

\begin{eqnarray}
\frac{\partial \text{Loss}}{\partial \mathbf{Y}} = \frac{1}{B} \mathbf{E}
\end{eqnarray}

To update the weights, we use the chain rule to express the gradient of the loss with respect to the weights $\mathbf{W}$ as follows:

\begin{eqnarray}
\frac{\partial \text{Loss}}{\partial \mathbf{W}} = \frac{\partial \text{Loss}}{\partial \mathbf{Y}} \cdot \frac{\partial \mathbf{Y}}{\partial \mathbf{W}}
\end{eqnarray}

Since we have $\frac{\partial \text{Loss}}{\partial \mathbf{Y}} = \frac{1}{B} \mathbf{E}$, we then calculate $\frac{\partial \mathbf{Y}}{\partial \mathbf{W}}$ by noting that $\mathbf{Y} = \mathbf{X} \mathbf{W}^T$. This is a matrix version of a familiar scalar derivative like $dy/dw(y = xw) = x$, and in a similar way the derivative of the matrix version is $\mathbf{X}$. 

Combining these derivatives gives us:

\begin{eqnarray}
\frac{\partial \text{Loss}}{\partial \mathbf{W}} = \frac{1}{B} \mathbf{E}^T \mathbf{X}
\end{eqnarray}
Here, $\mathbf{E}^T$ has a shape of $n_{out} \times B$, and $\mathbf{X}$ has a shape of $B \times n_{in}$. Multiplying these gives an $n_{out} \times n_{in}$ matrix, which aligns with the shape of $\mathbf{W}$ and represents the weight updates.

% Make this a separate section on decomposing this kind of product into a sum of outer products?
% This packs what we get in SGD into matrix math.  We are just adding the weight deltas together.  That is apparent in the summation below. So in a sense there is no dependency between rows. That is surprising when you just see this mass of numbers interacting.  And some of that carries over to transformers where the stacks of vectors maintain their identities through all the processors. See the matrix product discussion in linear algebra.
One helpful way to think of this formula is as a sum of outer products of the kind we saw in the vectorized hebb rule and the vectorized (but not batched) LMS rule. The matrix product $\mathbf{E}^T \mathbf{X}$ can be expanded as a sum of outer products of error vectors $\mathbf{e}_i$ (rows of $\mathbf{E}$) and input vectors $\mathbf{x}_i$ (rows of $\mathbf{X}$). When we index like that, this identity holds:
\begin{eqnarray}
\mathbf{E}^T \mathbf{X} = \sum_{i=1}^{B} \mathbf{e}_i^T \mathbf{x}_i
\end{eqnarray}
which gives us a sum of outer products.

For example, suppose we have
\begin{eqnarray*}
\mathbf{E} = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}, \quad \mathbf{X} = \begin{pmatrix} 2 & 3 \\ 4 & 5 \end{pmatrix}
\end{eqnarray*}
These are both row-based matrices. $\mathbf{E}$ has errors as rows (what we sometimes call ``row errors'') and $\mathbf{X}$ has inputs as rows. So it's a nice familiar way to conceptualize things. Then we can expand $\mathbf{E}^T \mathbf{X}$ in a way that makes it clear we are taking outer products of these and summing them to get the weight deltas:
\begin{eqnarray*}
\mathbf{E}^T \mathbf{X} = \mathbf{e}_1^T \mathbf{x}_1 + \mathbf{e}_2^T \mathbf{x}_2 = \begin{pmatrix} 1 \\ 2 \end{pmatrix} \begin{pmatrix} 2 & 3 \end{pmatrix} + \begin{pmatrix} 3 \\ 4 \end{pmatrix} \begin{pmatrix} 4 & 5 \end{pmatrix} = \begin{pmatrix} 2 & 3 \\ 4 & 6 \end{pmatrix} + \begin{pmatrix} 12 & 15 \\ 16 & 20 \end{pmatrix} = \begin{pmatrix} 14 & 18 \\ 20 & 26 \end{pmatrix}
\end{eqnarray*}

Now we can state the weight update rule for batched LMS without bias or activation rules. Using the learning rate $\epsilon$, and remembering that we are doing gradient descent so that the derivative has to be multiplied by $-1$, the weight change for the matrix in the batch case is:

\begin{eqnarray}
\Delta \mathbf{W} =  - \epsilon \frac{1}{B} \mathbf{E}^T \mathbf{X}
\end{eqnarray}
   
\subsection{Backprop and Automatic Differentiation}

As we move to learning rules for more than one weight layer, it is harder to interpret things directly, but we can abstractly understand them and also conceive of them in a kind of modular way, building off chain rules like the one above, 
\begin{eqnarray}
\frac{\partial \text{Loss}}{\partial \mathbf{W}} = \frac{\partial \text{Loss}}{\partial \mathbf{Y}} \cdot \frac{\partial \mathbf{Y}}{\partial \mathbf{W}}
\end{eqnarray}
Just with potentially way more steps than that.
% Integrate classroom notes and images on this
